{
  "id": "strike-speech-processing-pipeline",
  "name": "Complete Speech Processing Pipeline",
  "stack": [
    "typescript",
    "linguistics",
    "audio",
    "ai"
  ],
  "tags": [
    "speech",
    "pipeline",
    "stt",
    "tts",
    "phonetics",
    "prosody",
    "nlp",
    "audio"
  ],
  "description": "End-to-end speech processing pipeline: transcription â†’ phonetic analysis â†’ prosodic annotation â†’ synthesis with full X-SAMPA/IPA/ToBI support",
  "files": [
    {
      "path": "src/pipeline/speech-processor.ts",
      "template": "/**\n * Complete Speech Processing Pipeline\n * Integrates STT, phonetic analysis, prosodic annotation, and TTS\n */\n\nimport { xsampaConverter } from '../phonetics/xsampa-converter';\nimport { tobiAnnotator, ToBIAnnotation } from '../prosody/tobi-annotator';\nimport { prosodyAnalyzer, ProsodyAnalysis } from '../prosody/prosody-analyzer';\n\nexport interface AudioBuffer {\n  data: Float32Array;\n  sampleRate: number;\n  channels: number;\n  duration: number;\n}\n\nexport interface TranscriptionResult {\n  text: string;\n  confidence: number;\n  words: Array<{\n    word: string;\n    start: number;\n    end: number;\n    confidence: number;\n  }>;\n  phonemes?: string[];\n}\n\nexport interface SpeechFeatures {\n  pitch: number[];           // F0 contour\n  intensity: number[];       // RMS energy\n  formants: number[][];      // F1, F2, F3 formants\n  spectral: number[][];      // MFCC features\n  duration: number[];        // segment durations\n  timestamps: number[];      // temporal alignment\n}\n\nexport interface SpeechAnalysis {\n  transcription: TranscriptionResult;\n  phonetics: {\n    ipa: string;\n    xsampa: string;\n    features: any[];\n  };\n  prosody: {\n    tobi: ToBIAnnotation;\n    analysis: ProsodyAnalysis;\n  };\n  acoustics: SpeechFeatures;\n  metadata: {\n    language: string;\n    speaker?: string;\n    quality: number;\n    processing_time: number;\n  };\n}\n\nexport interface SynthesisOptions {\n  voice?: string;\n  rate?: number;              // speaking rate multiplier\n  pitch?: number;             // pitch shift in semitones\n  volume?: number;            // volume level (0-1)\n  emotion?: string;           // emotional coloring\n  prosody_control?: boolean;  // use ToBI for synthesis\n}\n\nexport class SpeechProcessor {\n  private readonly supportedLanguages = ['en-US', 'en-GB', 'es-ES', 'fr-FR', 'de-DE', 'ja-JP'];\n  private readonly defaultSampleRate = 16000;\n\n  /**\n   * Complete speech analysis pipeline\n   */\n  async analyzeSpeech(audioBuffer: AudioBuffer, options?: {\n    language?: string;\n    include_phonetics?: boolean;\n    include_prosody?: boolean;\n    include_acoustics?: boolean;\n  }): Promise<SpeechAnalysis> {\n    const startTime = Date.now();\n    const language = options?.language || 'en-US';\n    \n    // Step 1: Extract acoustic features\n    const acoustics = await this.extractAcousticFeatures(audioBuffer);\n    \n    // Step 2: Speech-to-text transcription\n    const transcription = await this.transcribeAudio(audioBuffer, language);\n    \n    // Step 3: Phonetic analysis\n    let phonetics = { ipa: '', xsampa: '', features: [] };\n    if (options?.include_phonetics !== false) {\n      phonetics = await this.analyzePhonetics(transcription.text, language);\n    }\n    \n    // Step 4: Prosodic annotation\n    let prosody = { tobi: {} as ToBIAnnotation, analysis: {} as ProsodyAnalysis };\n    if (options?.include_prosody !== false) {\n      const tobiAnnotation = tobiAnnotator.annotate(transcription.text, {\n        pitch: acoustics.pitch,\n        intensity: acoustics.intensity,\n        duration: acoustics.duration,\n        timestamps: acoustics.timestamps\n      }, {\n        language,\n        auto_detect_tones: true,\n        auto_detect_breaks: true\n      });\n      \n      prosody = {\n        tobi: tobiAnnotation,\n        analysis: prosodyAnalyzer.analyze(tobiAnnotation)\n      };\n    }\n    \n    const processingTime = Date.now() - startTime;\n    \n    return {\n      transcription,\n      phonetics,\n      prosody,\n      acoustics,\n      metadata: {\n        language,\n        quality: this.assessAudioQuality(audioBuffer),\n        processing_time: processingTime\n      }\n    };\n  }\n\n  /**\n   * Text-to-speech synthesis with prosodic control\n   */\n  async synthesizeSpeech(text: string, options?: SynthesisOptions): Promise<AudioBuffer> {\n    const voice = options?.voice || 'neutral';\n    const rate = options?.rate || 1.0;\n    const pitch = options?.pitch || 0;\n    const volume = options?.volume || 0.8;\n    \n    // Step 1: Text preprocessing\n    const normalizedText = this.normalizeText(text);\n    \n    // Step 2: Phonetic conversion\n    const phonetics = await this.analyzePhonetics(normalizedText);\n    \n    // Step 3: Prosodic planning\n    let prosodyPlan: ToBIAnnotation | undefined;\n    if (options?.prosody_control) {\n      // Generate prosodic plan for synthesis\n      prosodyPlan = await this.generateProsodyPlan(normalizedText, options.emotion);\n    }\n    \n    // Step 4: Acoustic synthesis\n    const audioBuffer = await this.synthesizeAudio({\n      text: normalizedText,\n      phonetics: phonetics.ipa,\n      prosody: prosodyPlan,\n      voice,\n      rate,\n      pitch,\n      volume\n    });\n    \n    return audioBuffer;\n  }\n\n  /**\n   * Convert between speech representations\n   */\n  convertSpeechNotation(input: string, from: 'text' | 'ipa' | 'xsampa' | 'tobi', to: 'text' | 'ipa' | 'xsampa' | 'tobi'): string {\n    if (from === to) return input;\n    \n    switch (`${from}->${to}`) {\n      case 'xsampa->ipa':\n        return xsampaConverter.xsampaToIpa(input);\n      case 'ipa->xsampa':\n        return xsampaConverter.ipaToXsampa(input);\n      case 'tobi->text':\n        const parsed = tobiAnnotator.parseToBIString(input);\n        return parsed.words?.join(' ') || '';\n      default:\n        throw new Error(`Unsupported conversion: ${from} -> ${to}`);\n    }\n  }\n\n  /**\n   * Batch process multiple audio files\n   */\n  async processBatch(audioFiles: AudioBuffer[], options?: {\n    language?: string;\n    parallel?: boolean;\n    progress_callback?: (progress: number) => void;\n  }): Promise<SpeechAnalysis[]> {\n    const results: SpeechAnalysis[] = [];\n    \n    if (options?.parallel) {\n      // Process in parallel\n      const promises = audioFiles.map(audio => this.analyzeSpeech(audio, { language: options.language }));\n      const batchResults = await Promise.all(promises);\n      results.push(...batchResults);\n    } else {\n      // Process sequentially with progress tracking\n      for (let i = 0; i < audioFiles.length; i++) {\n        const result = await this.analyzeSpeech(audioFiles[i], { language: options?.language });\n        results.push(result);\n        \n        if (options?.progress_callback) {\n          options.progress_callback((i + 1) / audioFiles.length);\n        }\n      }\n    }\n    \n    return results;\n  }\n\n  /**\n   * Real-time speech processing\n   */\n  createRealTimeProcessor(options?: {\n    language?: string;\n    chunk_size?: number;\n    overlap?: number;\n    callback?: (analysis: Partial<SpeechAnalysis>) => void;\n  }) {\n    const chunkSize = options?.chunk_size || 1024;\n    const overlap = options?.overlap || 256;\n    let buffer: Float32Array = new Float32Array(0);\n    \n    return {\n      process: async (audioChunk: Float32Array) => {\n        // Append to buffer\n        const newBuffer = new Float32Array(buffer.length + audioChunk.length);\n        newBuffer.set(buffer);\n        newBuffer.set(audioChunk, buffer.length);\n        buffer = newBuffer;\n        \n        // Process when we have enough data\n        if (buffer.length >= chunkSize) {\n          const processChunk = buffer.slice(0, chunkSize);\n          buffer = buffer.slice(chunkSize - overlap);\n          \n          const audioBuffer: AudioBuffer = {\n            data: processChunk,\n            sampleRate: this.defaultSampleRate,\n            channels: 1,\n            duration: processChunk.length / this.defaultSampleRate\n          };\n          \n          try {\n            const analysis = await this.analyzeSpeech(audioBuffer, {\n              language: options?.language,\n              include_acoustics: false // Skip for real-time\n            });\n            \n            options?.callback?.(analysis);\n          } catch (error) {\n            console.warn('Real-time processing error:', error);\n          }\n        }\n      },\n      \n      flush: async () => {\n        if (buffer.length > 0) {\n          const audioBuffer: AudioBuffer = {\n            data: buffer,\n            sampleRate: this.defaultSampleRate,\n            channels: 1,\n            duration: buffer.length / this.defaultSampleRate\n          };\n          \n          const analysis = await this.analyzeSpeech(audioBuffer, {\n            language: options?.language\n          });\n          \n          options?.callback?.(analysis);\n          buffer = new Float32Array(0);\n        }\n      }\n    };\n  }\n\n  // Private helper methods\n  private async extractAcousticFeatures(audioBuffer: AudioBuffer): Promise<SpeechFeatures> {\n    // Extract pitch using autocorrelation\n    const pitch = this.extractPitch(audioBuffer.data, audioBuffer.sampleRate);\n    \n    // Extract intensity (RMS energy)\n    const intensity = this.extractIntensity(audioBuffer.data);\n    \n    // Extract formants using LPC\n    const formants = this.extractFormants(audioBuffer.data, audioBuffer.sampleRate);\n    \n    // Extract MFCC features\n    const spectral = this.extractMFCC(audioBuffer.data, audioBuffer.sampleRate);\n    \n    // Generate timestamps\n    const frameSize = 256;\n    const hopSize = 128;\n    const timestamps = Array.from({ length: pitch.length }, (_, i) => i * hopSize / audioBuffer.sampleRate * 1000);\n    \n    return {\n      pitch,\n      intensity,\n      formants,\n      spectral,\n      duration: [audioBuffer.duration * 1000], // convert to ms\n      timestamps\n    };\n  }\n\n  private async transcribeAudio(audioBuffer: AudioBuffer, language: string): Promise<TranscriptionResult> {\n    // Mock implementation - replace with actual STT service\n    const mockWords = ['Hello', 'world', 'this', 'is', 'a', 'test'];\n    const wordsPerSecond = 3;\n    const wordDuration = 1000 / wordsPerSecond;\n    \n    return {\n      text: mockWords.join(' '),\n      confidence: 0.95,\n      words: mockWords.map((word, i) => ({\n        word,\n        start: i * wordDuration,\n        end: (i + 1) * wordDuration,\n        confidence: 0.9 + Math.random() * 0.1\n      }))\n    };\n  }\n\n  private async analyzePhonetics(text: string, language: string = 'en-US'): Promise<{ ipa: string; xsampa: string; features: any[] }> {\n    // Simple English phonemization - replace with proper library\n    const phoneticMap: Record<string, string> = {\n      'hello': 'hÉ›ËˆloÊŠ',\n      'world': 'wÉœËrld',\n      'this': 'Ã°Éªs',\n      'is': 'Éªz',\n      'test': 'tÉ›st'\n    };\n    \n    const words = text.toLowerCase().split(/\\s+/);\n    const ipaWords = words.map(word => phoneticMap[word] || word);\n    const ipa = ipaWords.join(' ');\n    const xsampa = xsampaConverter.ipaToXsampa(ipa);\n    \n    return {\n      ipa,\n      xsampa,\n      features: [] // TODO: extract phonetic features\n    };\n  }\n\n  private async generateProsodyPlan(text: string, emotion?: string): Promise<ToBIAnnotation> {\n    // Generate basic prosodic plan based on text and emotion\n    const mockFeatures = {\n      pitch: emotion === 'excited' ? [200, 250, 220, 180] : [150, 170, 160, 140],\n      intensity: [0.8, 0.9, 0.7, 0.6],\n      duration: emotion === 'fast' ? [80, 90, 85, 95] : [120, 130, 125, 135],\n      timestamps: [0, 100, 200, 300]\n    };\n    \n    return tobiAnnotator.annotate(text, mockFeatures, {\n      language: 'en-US',\n      auto_detect_tones: true,\n      auto_detect_breaks: true\n    });\n  }\n\n  private async synthesizeAudio(params: {\n    text: string;\n    phonetics: string;\n    prosody?: ToBIAnnotation;\n    voice: string;\n    rate: number;\n    pitch: number;\n    volume: number;\n  }): Promise<AudioBuffer> {\n    // Mock synthesis - replace with actual TTS engine\n    const duration = params.text.length * 0.1; // 100ms per character\n    const sampleRate = this.defaultSampleRate;\n    const samples = Math.floor(duration * sampleRate);\n    \n    // Generate simple sine wave based on prosody\n    const data = new Float32Array(samples);\n    const baseFreq = 150 * Math.pow(2, params.pitch / 12); // pitch shift\n    \n    for (let i = 0; i < samples; i++) {\n      const t = i / sampleRate;\n      const freq = baseFreq * (1 + 0.1 * Math.sin(2 * Math.PI * t * 2)); // slight vibrato\n      data[i] = params.volume * Math.sin(2 * Math.PI * freq * t) * Math.exp(-t * 2);\n    }\n    \n    return {\n      data,\n      sampleRate,\n      channels: 1,\n      duration\n    };\n  }\n\n  private normalizeText(text: string): string {\n    return text\n      .toLowerCase()\n      .replace(/[^\\w\\s.,!?]/g, '')\n      .replace(/\\s+/g, ' ')\n      .trim();\n  }\n\n  private assessAudioQuality(audioBuffer: AudioBuffer): number {\n    // Simple SNR estimation\n    const signal = audioBuffer.data;\n    const signalPower = signal.reduce((sum, x) => sum + x * x, 0) / signal.length;\n    const noisePower = Math.min(0.01, signalPower * 0.1); // estimate\n    const snr = 10 * Math.log10(signalPower / noisePower);\n    \n    return Math.max(0, Math.min(1, (snr - 10) / 30)); // normalize to 0-1\n  }\n\n  // Audio feature extraction helpers\n  private extractPitch(signal: Float32Array, sampleRate: number): number[] {\n    // Autocorrelation-based pitch detection\n    const frameSize = 1024;\n    const hopSize = 256;\n    const pitch: number[] = [];\n    \n    for (let i = 0; i < signal.length - frameSize; i += hopSize) {\n      const frame = signal.slice(i, i + frameSize);\n      const f0 = this.autocorrelationPitch(frame, sampleRate);\n      pitch.push(f0);\n    }\n    \n    return pitch;\n  }\n\n  private extractIntensity(signal: Float32Array): number[] {\n    const frameSize = 256;\n    const hopSize = 128;\n    const intensity: number[] = [];\n    \n    for (let i = 0; i < signal.length - frameSize; i += hopSize) {\n      const frame = signal.slice(i, i + frameSize);\n      const rms = Math.sqrt(frame.reduce((sum, x) => sum + x * x, 0) / frame.length);\n      intensity.push(rms);\n    }\n    \n    return intensity;\n  }\n\n  private extractFormants(signal: Float32Array, sampleRate: number): number[][] {\n    // LPC-based formant extraction (simplified)\n    const frameSize = 512;\n    const hopSize = 256;\n    const formants: number[][] = [];\n    \n    for (let i = 0; i < signal.length - frameSize; i += hopSize) {\n      const frame = signal.slice(i, i + frameSize);\n      // Mock formant values - replace with actual LPC analysis\n      formants.push([700, 1220, 2600]); // F1, F2, F3\n    }\n    \n    return formants;\n  }\n\n  private extractMFCC(signal: Float32Array, sampleRate: number): number[][] {\n    // MFCC feature extraction (simplified)\n    const frameSize = 512;\n    const hopSize = 256;\n    const numCoeffs = 13;\n    const mfcc: number[][] = [];\n    \n    for (let i = 0; i < signal.length - frameSize; i += hopSize) {\n      // Mock MFCC values - replace with actual MFCC computation\n      const coeffs = Array.from({ length: numCoeffs }, () => Math.random() * 2 - 1);\n      mfcc.push(coeffs);\n    }\n    \n    return mfcc;\n  }\n\n  private autocorrelationPitch(frame: Float32Array, sampleRate: number): number {\n    const minPeriod = Math.floor(sampleRate / 500); // 500 Hz max\n    const maxPeriod = Math.floor(sampleRate / 80);  // 80 Hz min\n    \n    let maxCorr = 0;\n    let bestPeriod = 0;\n    \n    for (let period = minPeriod; period <= maxPeriod; period++) {\n      let corr = 0;\n      let norm = 0;\n      \n      for (let i = 0; i < frame.length - period; i++) {\n        corr += frame[i] * frame[i + period];\n        norm += frame[i] * frame[i];\n      }\n      \n      if (norm > 0) {\n        corr /= norm;\n        if (corr > maxCorr) {\n          maxCorr = corr;\n          bestPeriod = period;\n        }\n      }\n    }\n    \n    return bestPeriod > 0 ? sampleRate / bestPeriod : 0;\n  }\n}\n\n// Export singleton instance\nexport const speechProcessor = new SpeechProcessor();\n"
    },
    {
      "path": "src/pipeline/speech-utils.ts",
      "template": "/**\n * Utility functions for speech processing\n */\n\nimport { AudioBuffer, SpeechAnalysis } from './speech-processor';\n\n/**\n * Audio format conversion utilities\n */\nexport class AudioUtils {\n  /**\n   * Convert WAV file to AudioBuffer\n   */\n  static async wavToAudioBuffer(wavData: ArrayBuffer): Promise<AudioBuffer> {\n    const view = new DataView(wavData);\n    \n    // Read WAV header\n    const sampleRate = view.getUint32(24, true);\n    const channels = view.getUint16(22, true);\n    const bitsPerSample = view.getUint16(34, true);\n    const dataStart = 44; // Standard WAV header size\n    \n    const bytesPerSample = bitsPerSample / 8;\n    const numSamples = (wavData.byteLength - dataStart) / (bytesPerSample * channels);\n    \n    const data = new Float32Array(numSamples);\n    \n    for (let i = 0; i < numSamples; i++) {\n      const offset = dataStart + i * bytesPerSample * channels;\n      \n      if (bitsPerSample === 16) {\n        const sample = view.getInt16(offset, true);\n        data[i] = sample / 32768.0;\n      } else if (bitsPerSample === 32) {\n        const sample = view.getFloat32(offset, true);\n        data[i] = sample;\n      }\n    }\n    \n    return {\n      data,\n      sampleRate,\n      channels,\n      duration: numSamples / sampleRate\n    };\n  }\n\n  /**\n   * Convert AudioBuffer to WAV file\n   */\n  static audioBufferToWav(audioBuffer: AudioBuffer): ArrayBuffer {\n    const { data, sampleRate, channels } = audioBuffer;\n    const length = data.length;\n    const buffer = new ArrayBuffer(44 + length * 2);\n    const view = new DataView(buffer);\n    \n    // WAV header\n    const writeString = (offset: number, string: string) => {\n      for (let i = 0; i < string.length; i++) {\n        view.setUint8(offset + i, string.charCodeAt(i));\n      }\n    };\n    \n    writeString(0, 'RIFF');\n    view.setUint32(4, 36 + length * 2, true);\n    writeString(8, 'WAVE');\n    writeString(12, 'fmt ');\n    view.setUint32(16, 16, true);\n    view.setUint16(20, 1, true);\n    view.setUint16(22, channels, true);\n    view.setUint32(24, sampleRate, true);\n    view.setUint32(28, sampleRate * channels * 2, true);\n    view.setUint16(32, channels * 2, true);\n    view.setUint16(34, 16, true);\n    writeString(36, 'data');\n    view.setUint32(40, length * 2, true);\n    \n    // Convert float samples to 16-bit PCM\n    let offset = 44;\n    for (let i = 0; i < length; i++) {\n      const sample = Math.max(-1, Math.min(1, data[i]));\n      view.setInt16(offset, sample * 0x7FFF, true);\n      offset += 2;\n    }\n    \n    return buffer;\n  }\n\n  /**\n   * Resample audio to target sample rate\n   */\n  static resample(audioBuffer: AudioBuffer, targetSampleRate: number): AudioBuffer {\n    const { data, sampleRate } = audioBuffer;\n    \n    if (sampleRate === targetSampleRate) {\n      return audioBuffer;\n    }\n    \n    const ratio = targetSampleRate / sampleRate;\n    const newLength = Math.floor(data.length * ratio);\n    const newData = new Float32Array(newLength);\n    \n    for (let i = 0; i < newLength; i++) {\n      const index = i / ratio;\n      const index1 = Math.floor(index);\n      const index2 = Math.min(index1 + 1, data.length - 1);\n      const fraction = index - index1;\n      \n      newData[i] = data[index1] * (1 - fraction) + data[index2] * fraction;\n    }\n    \n    return {\n      data: newData,\n      sampleRate: targetSampleRate,\n      channels: audioBuffer.channels,\n      duration: newLength / targetSampleRate\n    };\n  }\n\n  /**\n   * Apply noise reduction\n   */\n  static denoise(audioBuffer: AudioBuffer, intensity: number = 0.5): AudioBuffer {\n    const { data } = audioBuffer;\n    const denoisedData = new Float32Array(data.length);\n    \n    // Simple spectral subtraction\n    const windowSize = 256;\n    const overlap = 128;\n    \n    for (let i = 0; i < data.length; i++) {\n      if (i < windowSize) {\n        denoisedData[i] = data[i] * (1 - intensity * 0.5);\n      } else {\n        // Moving average filter\n        let sum = 0;\n        for (let j = Math.max(0, i - windowSize); j <= Math.min(data.length - 1, i + windowSize); j++) {\n          sum += data[j];\n        }\n        const avg = sum / (2 * windowSize + 1);\n        denoisedData[i] = data[i] - intensity * avg;\n      }\n    }\n    \n    return {\n      ...audioBuffer,\n      data: denoisedData\n    };\n  }\n\n  /**\n   * Normalize audio amplitude\n   */\n  static normalize(audioBuffer: AudioBuffer, targetLevel: number = 0.8): AudioBuffer {\n    const { data } = audioBuffer;\n    const maxAmplitude = Math.max(...data.map(Math.abs));\n    \n    if (maxAmplitude === 0) return audioBuffer;\n    \n    const gain = targetLevel / maxAmplitude;\n    const normalizedData = data.map(sample => sample * gain);\n    \n    return {\n      ...audioBuffer,\n      data: new Float32Array(normalizedData)\n    };\n  }\n}\n\n/**\n * Speech analysis visualization utilities\n */\nexport class VisualizationUtils {\n  /**\n   * Generate spectrogram data\n   */\n  static generateSpectrogram(audioBuffer: AudioBuffer, options?: {\n    windowSize?: number;\n    overlap?: number;\n    freqBins?: number;\n  }): number[][] {\n    const windowSize = options?.windowSize || 512;\n    const overlap = options?.overlap || 256;\n    const freqBins = options?.freqBins || 256;\n    \n    const { data, sampleRate } = audioBuffer;\n    const spectrogram: number[][] = [];\n    \n    for (let i = 0; i < data.length - windowSize; i += windowSize - overlap) {\n      const window = data.slice(i, i + windowSize);\n      const fftResult = this.fft(window);\n      const magnitudes = fftResult.slice(0, freqBins).map(Math.abs);\n      spectrogram.push(magnitudes);\n    }\n    \n    return spectrogram;\n  }\n\n  /**\n   * Simple FFT implementation for visualization\n   */\n  private static fft(signal: Float32Array): number[] {\n    const N = signal.length;\n    const result = new Array(N).fill(0);\n    \n    // Simple DFT for demonstration - replace with proper FFT\n    for (let k = 0; k < N; k++) {\n      let real = 0;\n      let imag = 0;\n      \n      for (let n = 0; n < N; n++) {\n        const angle = -2 * Math.PI * k * n / N;\n        real += signal[n] * Math.cos(angle);\n        imag += signal[n] * Math.sin(angle);\n      }\n      \n      result[k] = Math.sqrt(real * real + imag * imag);\n    }\n    \n    return result;\n  }\n\n  /**\n   * Generate waveform visualization data\n   */\n  static generateWaveform(audioBuffer: AudioBuffer, width: number = 800): number[] {\n    const { data } = audioBuffer;\n    const samplesPerPixel = Math.floor(data.length / width);\n    const waveform: number[] = [];\n    \n    for (let i = 0; i < width; i++) {\n      const start = i * samplesPerPixel;\n      const end = Math.min(start + samplesPerPixel, data.length);\n      \n      let max = 0;\n      for (let j = start; j < end; j++) {\n        max = Math.max(max, Math.abs(data[j]));\n      }\n      \n      waveform.push(max);\n    }\n    \n    return waveform;\n  }\n}\n\n/**\n * Performance measurement utilities\n */\nexport class PerformanceUtils {\n  private static measurements = new Map<string, number[]>();\n\n  /**\n   * Start timing an operation\n   */\n  static startTimer(operation: string): () => number {\n    const startTime = performance.now();\n    \n    return () => {\n      const duration = performance.now() - startTime;\n      \n      if (!this.measurements.has(operation)) {\n        this.measurements.set(operation, []);\n      }\n      \n      this.measurements.get(operation)!.push(duration);\n      return duration;\n    };\n  }\n\n  /**\n   * Get performance statistics\n   */\n  static getStats(operation: string): {\n    count: number;\n    mean: number;\n    min: number;\n    max: number;\n    std: number;\n  } | null {\n    const times = this.measurements.get(operation);\n    if (!times || times.length === 0) return null;\n    \n    const count = times.length;\n    const mean = times.reduce((a, b) => a + b) / count;\n    const min = Math.min(...times);\n    const max = Math.max(...times);\n    const variance = times.reduce((acc, time) => acc + Math.pow(time - mean, 2), 0) / count;\n    const std = Math.sqrt(variance);\n    \n    return { count, mean, min, max, std };\n  }\n\n  /**\n   * Reset measurements\n   */\n  static reset(operation?: string): void {\n    if (operation) {\n      this.measurements.delete(operation);\n    } else {\n      this.measurements.clear();\n    }\n  }\n}\n\n/**\n * Export utilities for speech analysis\n */\nexport class ExportUtils {\n  /**\n   * Export speech analysis to JSON\n   */\n  static exportToJSON(analysis: SpeechAnalysis): string {\n    return JSON.stringify(analysis, null, 2);\n  }\n\n  /**\n   * Export ToBI annotation to TextGrid format\n   */\n  static exportToTextGrid(analysis: SpeechAnalysis): string {\n    const { prosody } = analysis;\n    const duration = analysis.acoustics.timestamps[analysis.acoustics.timestamps.length - 1] / 1000;\n    \n    let textGrid = `File type = \"ooTextFile\"\\n`;\n    textGrid += `Object class = \"TextGrid\"\\n`;\n    textGrid += `\\n`;\n    textGrid += `xmin = 0\\n`;\n    textGrid += `xmax = ${duration}\\n`;\n    textGrid += `tiers? <exists>\\n`;\n    textGrid += `size = 3\\n`;\n    textGrid += `item []:\\n`;\n    \n    // Words tier\n    textGrid += `    item [1]:\\n`;\n    textGrid += `        class = \"IntervalTier\"\\n`;\n    textGrid += `        name = \"words\"\\n`;\n    textGrid += `        xmin = 0\\n`;\n    textGrid += `        xmax = ${duration}\\n`;\n    textGrid += `        intervals: size = ${prosody.tobi.words.length}\\n`;\n    \n    for (let i = 0; i < prosody.tobi.words.length; i++) {\n      const start = i / prosody.tobi.words.length * duration;\n      const end = (i + 1) / prosody.tobi.words.length * duration;\n      textGrid += `        intervals [${i + 1}]:\\n`;\n      textGrid += `            xmin = ${start}\\n`;\n      textGrid += `            xmax = ${end}\\n`;\n      textGrid += `            text = \"${prosody.tobi.words[i]}\"\\n`;\n    }\n    \n    // Add tones and breaks tiers...\n    \n    return textGrid;\n  }\n\n  /**\n   * Export phonetic analysis to ELAN format\n   */\n  static exportToELAN(analysis: SpeechAnalysis): string {\n    // ELAN EAF format implementation\n    const duration = analysis.acoustics.timestamps[analysis.acoustics.timestamps.length - 1];\n    \n    let eaf = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n`;\n    eaf += `<ANNOTATION_DOCUMENT>\\n`;\n    eaf += `    <HEADER MEDIA_FILE=\"\" TIME_UNITS=\"milliseconds\">\\n`;\n    eaf += `        <MEDIA_DESCRIPTOR MEDIA_URL=\"\" RELATIVE_MEDIA_URL=\"\"/>\\n`;\n    eaf += `    </HEADER>\\n`;\n    eaf += `    <TIME_ORDER>\\n`;\n    \n    // Add time slots\n    analysis.acoustics.timestamps.forEach((time, i) => {\n      eaf += `        <TIME_SLOT TIME_SLOT_ID=\"ts${i + 1}\" TIME_VALUE=\"${Math.round(time)}\"/>\\n`;\n    });\n    \n    eaf += `    </TIME_ORDER>\\n`;\n    eaf += `</ANNOTATION_DOCUMENT>\\n`;\n    \n    return eaf;\n  }\n}\n"
    },
    {
      "path": "test/speech-pipeline.test.ts",
      "template": "/**\n * Test suite for speech processing pipeline\n */\n\nimport { describe, it, expect, beforeEach } from 'vitest';\nimport { speechProcessor, AudioBuffer } from '../src/pipeline/speech-processor';\nimport { AudioUtils, VisualizationUtils, PerformanceUtils } from '../src/pipeline/speech-utils';\n\ndescribe('Speech Processing Pipeline', () => {\n  let mockAudioBuffer: AudioBuffer;\n  \n  beforeEach(() => {\n    // Create mock audio buffer\n    const sampleRate = 16000;\n    const duration = 2; // 2 seconds\n    const samples = sampleRate * duration;\n    const data = new Float32Array(samples);\n    \n    // Generate sine wave with varying frequency (simulating speech)\n    for (let i = 0; i < samples; i++) {\n      const t = i / sampleRate;\n      const freq = 150 + 50 * Math.sin(2 * Math.PI * t * 2); // varying pitch\n      data[i] = 0.5 * Math.sin(2 * Math.PI * freq * t) * Math.exp(-t * 0.5);\n    }\n    \n    mockAudioBuffer = {\n      data,\n      sampleRate,\n      channels: 1,\n      duration\n    };\n  });\n\n  it('should analyze speech completely', async () => {\n    const analysis = await speechProcessor.analyzeSpeech(mockAudioBuffer, {\n      language: 'en-US',\n      include_phonetics: true,\n      include_prosody: true,\n      include_acoustics: true\n    });\n    \n    expect(analysis.transcription).toBeDefined();\n    expect(analysis.transcription.text).toBeTruthy();\n    expect(analysis.transcription.confidence).toBeGreaterThan(0);\n    \n    expect(analysis.phonetics).toBeDefined();\n    expect(analysis.phonetics.ipa).toBeTruthy();\n    expect(analysis.phonetics.xsampa).toBeTruthy();\n    \n    expect(analysis.prosody.tobi).toBeDefined();\n    expect(analysis.prosody.tobi.words.length).toBeGreaterThan(0);\n    \n    expect(analysis.acoustics).toBeDefined();\n    expect(analysis.acoustics.pitch.length).toBeGreaterThan(0);\n    expect(analysis.acoustics.intensity.length).toBeGreaterThan(0);\n    \n    expect(analysis.metadata.processing_time).toBeGreaterThan(0);\n    expect(analysis.metadata.quality).toBeGreaterThanOrEqual(0);\n    expect(analysis.metadata.quality).toBeLessThanOrEqual(1);\n  });\n\n  it('should synthesize speech with prosodic control', async () => {\n    const synthesized = await speechProcessor.synthesizeSpeech('Hello world', {\n      voice: 'neutral',\n      rate: 1.0,\n      pitch: 0,\n      volume: 0.8,\n      emotion: 'neutral',\n      prosody_control: true\n    });\n    \n    expect(synthesized).toBeDefined();\n    expect(synthesized.data.length).toBeGreaterThan(0);\n    expect(synthesized.sampleRate).toBe(16000);\n    expect(synthesized.channels).toBe(1);\n    expect(synthesized.duration).toBeGreaterThan(0);\n  });\n\n  it('should convert between speech notations', () => {\n    expect(speechProcessor.convertSpeechNotation('S', 'xsampa', 'ipa')).toBe('Êƒ');\n    expect(speechProcessor.convertSpeechNotation('Êƒ', 'ipa', 'xsampa')).toBe('S');\n    expect(speechProcessor.convertSpeechNotation('hello', 'text', 'text')).toBe('hello');\n  });\n\n  it('should process batch audio files', async () => {\n    const audioFiles = [mockAudioBuffer, mockAudioBuffer, mockAudioBuffer];\n    const results = await speechProcessor.processBatch(audioFiles, {\n      language: 'en-US',\n      parallel: true\n    });\n    \n    expect(results.length).toBe(3);\n    results.forEach(result => {\n      expect(result.transcription).toBeDefined();\n      expect(result.metadata.language).toBe('en-US');\n    });\n  });\n\n  it('should create real-time processor', async () => {\n    let callbackCount = 0;\n    const processor = speechProcessor.createRealTimeProcessor({\n      language: 'en-US',\n      chunk_size: 1024,\n      overlap: 256,\n      callback: (analysis) => {\n        expect(analysis.transcription).toBeDefined();\n        callbackCount++;\n      }\n    });\n    \n    // Process chunks\n    const chunkSize = 1024;\n    for (let i = 0; i < mockAudioBuffer.data.length; i += chunkSize) {\n      const chunk = mockAudioBuffer.data.slice(i, i + chunkSize);\n      await processor.process(chunk);\n    }\n    \n    await processor.flush();\n    expect(callbackCount).toBeGreaterThan(0);\n  });\n});\n\ndescribe('Audio Utils', () => {\n  it('should resample audio', () => {\n    const original: AudioBuffer = {\n      data: new Float32Array([1, 0, -1, 0, 1, 0, -1, 0]),\n      sampleRate: 8000,\n      channels: 1,\n      duration: 1\n    };\n    \n    const resampled = AudioUtils.resample(original, 16000);\n    expect(resampled.sampleRate).toBe(16000);\n    expect(resampled.data.length).toBeGreaterThan(original.data.length);\n  });\n\n  it('should denoise audio', () => {\n    const noisy: AudioBuffer = {\n      data: new Float32Array([0.8, 0.1, -0.7, 0.05, 0.9, -0.02]),\n      sampleRate: 16000,\n      channels: 1,\n      duration: 1\n    };\n    \n    const denoised = AudioUtils.denoise(noisy, 0.5);\n    expect(denoised.data.length).toBe(noisy.data.length);\n    expect(Math.abs(denoised.data[0])).toBeLessThan(Math.abs(noisy.data[0]));\n  });\n\n  it('should normalize audio', () => {\n    const quiet: AudioBuffer = {\n      data: new Float32Array([0.1, -0.1, 0.05, -0.05]),\n      sampleRate: 16000,\n      channels: 1,\n      duration: 1\n    };\n    \n    const normalized = AudioUtils.normalize(quiet, 0.8);\n    const maxAmplitude = Math.max(...normalized.data.map(Math.abs));\n    expect(maxAmplitude).toBeCloseTo(0.8, 1);\n  });\n});\n\ndescribe('Visualization Utils', () => {\n  it('should generate spectrogram', () => {\n    const mockAudio: AudioBuffer = {\n      data: new Float32Array(Array.from({ length: 1024 }, (_, i) => Math.sin(2 * Math.PI * 440 * i / 16000))),\n      sampleRate: 16000,\n      channels: 1,\n      duration: 1\n    };\n    \n    const spectrogram = VisualizationUtils.generateSpectrogram(mockAudio, {\n      windowSize: 256,\n      overlap: 128,\n      freqBins: 128\n    });\n    \n    expect(spectrogram.length).toBeGreaterThan(0);\n    expect(spectrogram[0].length).toBe(128);\n  });\n\n  it('should generate waveform', () => {\n    const mockAudio: AudioBuffer = {\n      data: new Float32Array([0.5, 1.0, -0.5, -1.0, 0.0]),\n      sampleRate: 16000,\n      channels: 1,\n      duration: 1\n    };\n    \n    const waveform = VisualizationUtils.generateWaveform(mockAudio, 100);\n    expect(waveform.length).toBe(100);\n    expect(Math.max(...waveform)).toBeGreaterThan(0);\n  });\n});\n\ndescribe('Performance Utils', () => {\n  it('should measure operation timing', () => {\n    const stopTimer = PerformanceUtils.startTimer('test-operation');\n    \n    // Simulate work\n    const result = Array.from({ length: 1000 }, (_, i) => i * i).reduce((a, b) => a + b);\n    \n    const duration = stopTimer();\n    expect(duration).toBeGreaterThan(0);\n    \n    const stats = PerformanceUtils.getStats('test-operation');\n    expect(stats?.count).toBe(1);\n    expect(stats?.mean).toBe(duration);\n  });\n\n  it('should track multiple measurements', () => {\n    // Run multiple times\n    for (let i = 0; i < 5; i++) {\n      const stopTimer = PerformanceUtils.startTimer('multi-test');\n      // Simulate variable work\n      setTimeout(() => {}, i); \n      stopTimer();\n    }\n    \n    const stats = PerformanceUtils.getStats('multi-test');\n    expect(stats?.count).toBe(5);\n    expect(stats?.mean).toBeGreaterThan(0);\n  });\n});\n"
    },
    {
      "path": "package.json",
      "template": "{\n  \"name\": \"speech-processing-pipeline\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Complete speech processing pipeline with phonetic analysis and prosodic annotation\",\n  \"type\": \"module\",\n  \"main\": \"dist/index.js\",\n  \"types\": \"dist/index.d.ts\",\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"test\": \"vitest\",\n    \"dev\": \"tsc --watch\",\n    \"lint\": \"eslint src/**/*.ts\",\n    \"demo\": \"tsx examples/demo.ts\"\n  },\n  \"dependencies\": {\n    \"fft-js\": \"^0.0.12\",\n    \"ml-matrix\": \"^6.10.4\",\n    \"wav\": \"^1.0.2\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.0.0\",\n    \"@types/wav\": \"^1.0.0\",\n    \"typescript\": \"^5.0.0\",\n    \"vitest\": \"^1.0.0\",\n    \"tsx\": \"^4.0.0\",\n    \"eslint\": \"^8.0.0\",\n    \"@typescript-eslint/eslint-plugin\": \"^6.0.0\",\n    \"@typescript-eslint/parser\": \"^6.0.0\"\n  },\n  \"keywords\": [\n    \"speech\",\n    \"processing\",\n    \"pipeline\",\n    \"stt\",\n    \"tts\",\n    \"phonetics\",\n    \"prosody\",\n    \"x-sampa\",\n    \"ipa\",\n    \"tobi\",\n    \"linguistics\",\n    \"audio\",\n    \"nlp\"\n  ]\n}\n"
    },
    {
      "path": "README.md",
      "template": "# Complete Speech Processing Pipeline\n\nA comprehensive TypeScript library for end-to-end speech processing, integrating speech-to-text, phonetic analysis, prosodic annotation, and text-to-speech synthesis.\n\n## Features\n\nðŸŽ¤ **Speech-to-Text**: Advanced transcription with word-level timing\nðŸ”¤ **Phonetic Analysis**: X-SAMPA/IPA conversion and feature extraction\nðŸŽµ **Prosodic Annotation**: ToBI system for intonation and rhythm analysis\nðŸ—£ï¸ **Text-to-Speech**: Synthesis with prosodic control\nðŸ“Š **Audio Analysis**: Pitch, formants, spectral features extraction\nâš¡ **Real-time Processing**: Streaming audio analysis\nðŸŽ¯ **Multi-language**: Support for multiple languages and dialects\n\n## Installation\n\n```bash\nnpm install speech-processing-pipeline\n```\n\n## Quick Start\n\n### Complete Speech Analysis\n\n```typescript\nimport { speechProcessor, AudioUtils } from 'speech-processing-pipeline';\n\n// Load audio file\nconst wavData = await fs.readFile('speech.wav');\nconst audioBuffer = await AudioUtils.wavToAudioBuffer(wavData.buffer);\n\n// Analyze speech\nconst analysis = await speechProcessor.analyzeSpeech(audioBuffer, {\n  language: 'en-US',\n  include_phonetics: true,\n  include_prosody: true,\n  include_acoustics: true\n});\n\nconsole.log('Transcription:', analysis.transcription.text);\nconsole.log('IPA:', analysis.phonetics.ipa);\nconsole.log('X-SAMPA:', analysis.phonetics.xsampa);\nconsole.log('Prosody:', analysis.prosody.tobi.tones);\nconsole.log('Rhythm:', analysis.prosody.analysis.rhythm.tempo, 'BPM');\n```\n\n### Text-to-Speech Synthesis\n\n```typescript\n// Synthesize speech with prosodic control\nconst audioBuffer = await speechProcessor.synthesizeSpeech('Hello world!', {\n  voice: 'neutral',\n  rate: 1.2,           // 20% faster\n  pitch: 2,            // 2 semitones higher\n  volume: 0.8,\n  emotion: 'excited',\n  prosody_control: true // Use ToBI for natural intonation\n});\n\n// Convert to WAV and save\nconst wavData = AudioUtils.audioBufferToWav(audioBuffer);\nawait fs.writeFile('output.wav', Buffer.from(wavData));\n```\n\n### Real-time Processing\n\n```typescript\n// Create real-time processor\nconst processor = speechProcessor.createRealTimeProcessor({\n  language: 'en-US',\n  callback: (analysis) => {\n    console.log('Real-time transcription:', analysis.transcription?.text);\n    console.log('Current pitch:', analysis.acoustics?.pitch?.slice(-1)[0], 'Hz');\n  }\n});\n\n// Process audio chunks\nmicrophone.on('data', async (chunk) => {\n  await processor.process(chunk);\n});\n```\n\n### Phonetic Conversion\n\n```typescript\n// Convert between phonetic notations\nconst ipa = speechProcessor.convertSpeechNotation('hE\"loU', 'xsampa', 'ipa');\nconsole.log(ipa); // 'hÉ›ËˆloÊŠ'\n\nconst xsampa = speechProcessor.convertSpeechNotation('Êƒ', 'ipa', 'xsampa');\nconsole.log(xsampa); // 'S'\n```\n\n### Batch Processing\n\n```typescript\n// Process multiple files\nconst audioFiles = await Promise.all([\n  AudioUtils.wavToAudioBuffer(file1),\n  AudioUtils.wavToAudioBuffer(file2),\n  AudioUtils.wavToAudioBuffer(file3)\n]);\n\nconst results = await speechProcessor.processBatch(audioFiles, {\n  language: 'en-US',\n  parallel: true,\n  progress_callback: (progress) => {\n    console.log(`Processing: ${Math.round(progress * 100)}%`);\n  }\n});\n```\n\n## Pipeline Components\n\n### 1. Acoustic Feature Extraction\n\n```typescript\n// Extract detailed acoustic features\nconst features = analysis.acoustics;\nconsole.log('Pitch contour:', features.pitch);\nconsole.log('Formants:', features.formants); // F1, F2, F3\nconsole.log('MFCC coefficients:', features.spectral);\nconsole.log('Intensity:', features.intensity);\n```\n\n### 2. Phonetic Analysis\n\n```typescript\n// Detailed phonetic breakdown\nconst phonetics = analysis.phonetics;\nconsole.log('IPA transcription:', phonetics.ipa);\nconsole.log('X-SAMPA notation:', phonetics.xsampa);\nconsole.log('Phonetic features:', phonetics.features);\n```\n\n### 3. Prosodic Annotation (ToBI)\n\n```typescript\n// ToBI prosodic annotation\nconst tobi = analysis.prosody.tobi;\nconsole.log('Pitch accents:', tobi.tones.filter(t => t.type.includes('*')));\nconsole.log('Break indices:', tobi.breaks);\nconsole.log('Phrase boundaries:', tobi.breaks.filter(b => b.level >= 3));\n\n// Prosodic analysis\nconst prosodyAnalysis = analysis.prosody.analysis;\nconsole.log('Speaking rate:', prosodyAnalysis.speaking_style.rate);\nconsole.log('Detected emotion:', prosodyAnalysis.speaking_style.emotion);\nconsole.log('Rhythm regularity:', prosodyAnalysis.rhythm.regularity);\n```\n\n## Audio Utilities\n\n### Format Conversion\n\n```typescript\nimport { AudioUtils } from 'speech-processing-pipeline';\n\n// WAV file handling\nconst audioBuffer = await AudioUtils.wavToAudioBuffer(wavArrayBuffer);\nconst wavData = AudioUtils.audioBufferToWav(audioBuffer);\n\n// Resample audio\nconst resampled = AudioUtils.resample(audioBuffer, 22050);\n\n// Audio enhancement\nconst denoised = AudioUtils.denoise(audioBuffer, 0.5);\nconst normalized = AudioUtils.normalize(audioBuffer, 0.8);\n```\n\n### Visualization\n\n```typescript\nimport { VisualizationUtils } from 'speech-processing-pipeline';\n\n// Generate spectrogram data\nconst spectrogram = VisualizationUtils.generateSpectrogram(audioBuffer, {\n  windowSize: 512,\n  overlap: 256,\n  freqBins: 256\n});\n\n// Generate waveform for visualization\nconst waveform = VisualizationUtils.generateWaveform(audioBuffer, 800);\n```\n\n## Export Formats\n\n### JSON Export\n\n```typescript\nimport { ExportUtils } from 'speech-processing-pipeline';\n\n// Export complete analysis to JSON\nconst jsonData = ExportUtils.exportToJSON(analysis);\nawait fs.writeFile('analysis.json', jsonData);\n```\n\n### Praat TextGrid\n\n```typescript\n// Export for Praat analysis\nconst textGrid = ExportUtils.exportToTextGrid(analysis);\nawait fs.writeFile('annotation.TextGrid', textGrid);\n```\n\n### ELAN Format\n\n```typescript\n// Export for ELAN annotation\nconst elanData = ExportUtils.exportToELAN(analysis);\nawait fs.writeFile('annotation.eaf', elanData);\n```\n\n## Performance Monitoring\n\n```typescript\nimport { PerformanceUtils } from 'speech-processing-pipeline';\n\n// Monitor processing performance\nconst stopTimer = PerformanceUtils.startTimer('speech-analysis');\nconst analysis = await speechProcessor.analyzeSpeech(audioBuffer);\nconst duration = stopTimer();\n\n// Get performance statistics\nconst stats = PerformanceUtils.getStats('speech-analysis');\nconsole.log(`Average processing time: ${stats.mean.toFixed(2)}ms`);\nconsole.log(`Processed ${stats.count} files`);\n```\n\n## Supported Languages\n\n- English (US, UK)\n- Spanish (Spain)\n- French (France)\n- German (Germany)\n- Japanese (Japan)\n\n## API Reference\n\n### SpeechProcessor\n\n- `analyzeSpeech(audioBuffer, options)` - Complete speech analysis\n- `synthesizeSpeech(text, options)` - Text-to-speech synthesis\n- `convertSpeechNotation(input, from, to)` - Notation conversion\n- `processBatch(audioFiles, options)` - Batch processing\n- `createRealTimeProcessor(options)` - Real-time processing\n\n### AudioUtils\n\n- `wavToAudioBuffer(wavData)` - Convert WAV to AudioBuffer\n- `audioBufferToWav(audioBuffer)` - Convert AudioBuffer to WAV\n- `resample(audioBuffer, targetSampleRate)` - Resample audio\n- `denoise(audioBuffer, intensity)` - Noise reduction\n- `normalize(audioBuffer, targetLevel)` - Amplitude normalization\n\n### Data Types\n\n```typescript\ninterface SpeechAnalysis {\n  transcription: TranscriptionResult;\n  phonetics: { ipa: string; xsampa: string; features: any[] };\n  prosody: { tobi: ToBIAnnotation; analysis: ProsodyAnalysis };\n  acoustics: SpeechFeatures;\n  metadata: { language: string; quality: number; processing_time: number };\n}\n\ninterface AudioBuffer {\n  data: Float32Array;\n  sampleRate: number;\n  channels: number;\n  duration: number;\n}\n```\n\n## Examples\n\nSee the `examples/` directory for complete usage examples:\n\n- `basic-analysis.ts` - Basic speech analysis\n- `real-time-processing.ts` - Real-time speech processing\n- `batch-processing.ts` - Batch file processing\n- `synthesis-demo.ts` - Text-to-speech synthesis\n- `phonetic-conversion.ts` - Phonetic notation conversion\n\n## Contributing\n\nContributions welcome! Please run tests and follow coding standards:\n\n```bash\nnpm test\nnpm run lint\n```\n\n## License\n\nMIT License - see LICENSE file for details.\n"
    }
  ],
  "patches": []
}