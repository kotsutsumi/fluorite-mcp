{
  "id": "strike-vllm-server-infer",
  "name": "vLLM Server Inference",
  "stack": [
    "python",
    "vllm"
  ],
  "tags": [
    "ml"
  ],
  "files": [
    {
      "path": "ai/vllm_infer.py",
      "template": "# vllm inference skeleton\n"
    }
  ],
  "patches": []
}
